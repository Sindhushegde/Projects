{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a4f9f18",
   "metadata": {},
   "source": [
    "## Building a Predictive Sales Dashboard with Amazon Redshift, QuickSight, and SageMaker.\n",
    "\n",
    "Description: The project will involve building a predictive sales dashboard using Amazon Redshift for data storage, Amazon QuickSight for data visualization, and Amazon SageMaker for building a machine learning model to predict future sales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23a864",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0e3c5",
   "metadata": {},
   "source": [
    "#### 1. Create an Amazon Redshift cluster to store the sales data.\n",
    "\n",
    "1. Open the Amazon Redshift console and click on the \"Create cluster\" button.\n",
    "\n",
    "2. Choose the \"Standard\" cluster option and configure the cluster settings, such as the cluster type, node type, number of nodes, and cluster identifier.\n",
    "\n",
    "3. Configure the network settings for the cluster, such as the VPC, subnet group, and security group.\n",
    "\n",
    "4. Choose the authentication settings for the cluster, such as the master user name and password.\n",
    "\n",
    "5. Enable advanced options and configure the cluster settings, such as the database options, maintenance settings, and backup settings.\n",
    "\n",
    "6. Review the cluster settings and click on the \"Create cluster\" button to create the cluster.\n",
    "\n",
    "7. Once the cluster is created, click on the cluster identifier to view the cluster details.\n",
    "\n",
    "8. Connect to the cluster using a SQL client, such as SQL Workbench/J, and create a database for the sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e93848",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE DATABASE sales;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cc7e3",
   "metadata": {},
   "source": [
    "9. Create tables for the sales data using the CREATE TABLE statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE sales (\n",
    "    date DATE,\n",
    "    product VARCHAR(50),\n",
    "    revenue FLOAT\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e0d27",
   "metadata": {},
   "source": [
    "10. Load the sales data into the table using the COPY command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8e46e",
   "metadata": {},
   "source": [
    "COPY sales FROM 's3://my-bucket/sales-data.csv'\n",
    "    CREDENTIALS 'aws_iam_role=arn:aws:iam::123456789012:role/my-redshift-role'\n",
    "    CSV;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10cc2c",
   "metadata": {},
   "source": [
    "Here, 'sales-data-bucket' is the name of the Amazon S3 bucket where the sales data is stored, and 'RedshiftRole' is the name of the IAM role that has the necessary permissions to access the Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb4703",
   "metadata": {},
   "source": [
    "#### 2. Create an Amazon SageMaker notebook instance to build a machine learning model to predict future sales:\n",
    "1. Open the Amazon SageMaker console and select \"Notebook instances\" from the navigation menu.\n",
    "\n",
    "2. Click on the \"Create notebook instance\" button.\n",
    "\n",
    "3. Enter a name for the notebook instance, choose an instance type, and select the VPC and security group for the notebook instance.\n",
    "\n",
    "4. Enable data encryption and choose an Amazon S3 bucket to store the notebook instance data.\n",
    "\n",
    "5. Choose a role that provides the necessary permissions to access the required AWS services, such as Amazon S3 and Amazon Redshift.\n",
    "\n",
    "6. Click on the \"Create notebook instance\" button to create the notebook instance.\n",
    "\n",
    "7. Once the notebook instance is created, click on the \"Open Jupyter\" button to launch the Jupyter notebook interface.\n",
    "\n",
    "8. Create a new notebook and import the necessary Python libraries, such as numpy, pandas, scikit-learn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f867d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('my-bucket')\n",
    "\n",
    "sales_data = pd.read_csv('s3://my-bucket/sales-data.csv')\n",
    "\n",
    "X = sales_data.drop(['revenue'], axis=1)\n",
    "y = sales_data['revenue']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save the model to S3\n",
    "model_key = 'sales-model.pkl'\n",
    "model_path = '/opt/ml/model/' + model_key\n",
    "joblib.dump(model, model_path)\n",
    "bucket.upload_file(model_path, model_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a7c25",
   "metadata": {},
   "source": [
    "#### 3. Create an Amazon QuickSight dashboard to visualize the sales data and the predictions from the machine learning model.\n",
    "\n",
    "1. Open the Amazon QuickSight console and select \"Manage data\" from the navigation menu.\n",
    "\n",
    "2. Select \"New data set\" and choose \"Amazon Redshift\" as the data source.\n",
    "\n",
    "3. Enter the Redshift cluster details and the credentials for the master user.\n",
    "\n",
    "4. Choose the sales table created earlier as the data source.\n",
    "\n",
    "5. Select \"Visualize\" from the navigation menu to open the Amazon QuickSight visualization editor.\n",
    "\n",
    "6. Drag and drop the \"date\" and \"revenue\" fields from the sales table to create a line chart that shows the historical sales data.\n",
    "7. Create a new calculated field to predict future sales using the machine learning model in Amazon SageMaker. You can use the \"predict\" method from the trained machine learning model to generate the predictions.\n",
    "\n",
    "     ##### predict('sagemaker-model-endpoint', 'sagemaker-model', [date_value])[0]\n",
    "\n",
    "      Here, 'sagemaker-model-endpoint' is the Amazon SageMaker endpoint for the deployed machine learning model, 'sagemaker-model' is the name of the model, and 'date_value' is the date for which you want to predict the sales.\n",
    "\n",
    "8. Add a new line to the chart to display the predicted sales data. Set the data source for the new line to the calculated field that you created in the previous step.\n",
    "9. Customize the chart properties, such as the colors, axis labels, and legends, to improve the readability of the chart.\n",
    "10. Add other charts and widgets to the dashboard to provide additional insights into the sales data, such as a bar chart that shows the sales by product, or a table that shows the top selling products.\n",
    "11. Save the dashboard and share it with other users in your organization. You can also schedule the dashboard to refresh the data at regular intervals, such as daily or weekly, to ensure that the dashboard always shows the latest data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526039be",
   "metadata": {},
   "source": [
    "Note: This project is suitable for small data because it involves analyzing and predicting sales for a small number of products. However, it can be easily scaled to handle larger data volumes by using AWS services like AWS Glue for ETL and AWS EMR for distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba589f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
